

## 03迭代优化方法



## 04 主成分分析PCA

无监督特征提取

### 特征提取

特征提取（降维/特征降维）指的是将原始的高维的数据映射到低维的空间

特征提取的标准会根据问题设置而不同

无监督设置——最大限度的减少信息的损失

监督设置——最大限度的分类区别

### 特征提取vs特征选择

特征提取：所有原始数据都被使用，变换过后的特征是原始特征的线性组合

特征选择：只是原始数据的子集

### 特征提取

可视化：高维数据投影到2D3D

数据压缩：高效村塾和检索

噪音去除：对准确定有积极影响

### 特征提取算法

无监督：

1. 主成分分析（PCA）
2. 非负矩阵分解（NMF）
3. 独立成分分析（ICA）

有监督：

1. 线性判别分析（LDA）
2. 通用图嵌入（GE）
3. 典型相关分析（CCA）

半监督：

1. 研究课题

### 数据降维

降维就是一种对高维度特征数据预处理方法。降维是将高维度的数据保留下最重要的一些特征，去除噪声和不重要的特征，从而实现提升数据处理速度的目的

降维具有如下一些优点：

- 使得数据集更易使用。
- 降低算法的计算开销。
- 去除噪声。
- 使得结果容易理解。

降维的算法有很多，比如奇异值分解（SVD）、主成分分析(PCA)、因子分析(FA)、独立成分分析(ICA)。



### 主成分分析

主成分分析是一种广泛用于降维，有损数据压缩，特征提取和数据可视化等应用的技术

将原始高维数据映射到低维

PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。

常用的定义：

最大化方差公式——投影数据方差最大

最小误差公式——最小平均预测成本

- 最近重构性——样本点到这个超平面的距离都足够近maximum variance formulation 
- 最大可分性——样本点在这个超平面上的投影尽可能分开minimum-error firmulation



<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419160921760.png" alt="image-20230419160921760" style="zoom:67%;" />



### 最大方差公式maximum variance formulation 

主成分分析的核心是降低由大量变量组成的数据集的位数，同时尽可能保留数据集中的存在的变化。

这是通过转换为**一组新的变量**，即**主成分(PCs)**来实现的，**它们是不相关的**，并按每个保留的总信息的比例排序，因此前几个保留了所有原始变量中存在的大部分变化。



### 主成分的代数推导

给定一组样本𝑚 对向量的观测𝑑 变量$\{x_1,x_2,…,x_m\}∈R^d$

通过线性投影定义样本的第一个PC$𝒘_1∈ℝ^𝑑$

把公式里的z看成y，w是 一个线性变换，将x变换到z，var[z1]表示z1方差，当var[z1]最大时取w,

![image-20230419175315511](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419175315511.png)

把var[z1]展开，发现$var[z1]=w_1^T Sw_s$,其中s是协方差矩阵， x ̅表示均值，

![image-20230419175621282](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419175621282.png)

协方差矩阵s是对称的。特征向量必须正交，s的特征值必须大于0

主要性质：

阐述中体主成分于协方差矩阵的特征值和特征向量的管=关系，同时给出一个求主成分的方法

**定理：**

设x是m维随机变量，$\sum$是x的协方差举证，$\sum$的特征值分别是λ1>=λ2>=λ3...>=λm>=0,特征值对应的单位特征向量分别是$\alpha$1，$\alpha$2，$\alpha$3...$\alpha$m,则x的第k主成分是：

$y_k=\alpha_k^Tx=\alpha_{1k}x_1+\alpha_{2k}x_2+...+\alpha_{mk}x_m,k=1,2...m$

x的第k主成分的方差是：

$var[y_k]=\alpha_k^T\sum{a_k}=λ_k$

即协方差矩阵的第k个特征值



采用拉格朗日乘子法求出主成分：

第一主成分，求解约束最优化问题

s.t.$𝒘_1^𝑻 𝒘_1=1$ max$𝑣𝑎𝑟[𝑧_1]$=$w_1^TSw_1$



查找$𝒘_1$最大化$𝑣𝑎𝑟[𝑧_1]$        在$𝒘_1^𝑻 𝒘_1=1$的情况下，求方差最大的，得到x的第一主成分

定义拉格朗日乘子，最大下面函数：![image-20230419193716559](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419193716559.png)

所以，最优化问题的解是：$w^Tx$构成第一主成分。其方差等于协方差举证的最大特征值。



对于第二主成分

不相关约束可以表示为：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419204914100.png" alt="image-20230419204914100" style="zoom:8=50%;" />

利用拉格朗日乘数，我们将最大化：<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419205025575.png" alt="image-20230419205025575" style="zoom:50%;" />

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419205107660.png" alt="image-20230419205107660" style="zoom:50%;" />

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419205125140.png" alt="image-20230419205125140" style="zoom:50%;" />

我们左乘W1得到：<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419205233589.png" alt="image-20230419205233589" style="zoom:50%;" />

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419205514202.png" alt="image-20230419205514202" style="zoom:50%;" />

φ必须为零，然后重新检查导数，

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419205335604.png" alt="image-20230419205335604" style="zoom:50%;" />

相同的选择策略$𝒘_2$作为与第二大特征值相关联的特征向量$𝜆_2$产生第二个PC。

此过程可以重复𝑘=1,…,𝑝 屈服于𝑝 的不同特征向量𝑺 以及相应的特征值𝜆_1,…𝜆_𝑝

每个PC的方差由下式给出<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419205717658.png" alt="image-20230419205717658" style="zoom:50%;" />



**PCA算法步骤**

输入：样本集D={x1,x2...xm}

过程：

1. 对所有样本进行去中心化$x_i$<—$x_i$—$\frac{1}{m}\sum_{i=1}{m}x_i$
2. 计算样本的协方差举证$XX^T$
3. 对协方差矩阵$XX^T$做特征值分解
4. 取最大的d`个特征值所对应的特征向量w1,w2....wd

输出：投影矩阵$W^*=(w_1,w_2...w_d)$

### 最小误差公式minimum-error firmulation











### 协方差和相关系数

对于两个变量x和y

协方差cov和相关系数如下

![image-20230419175818298](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419175818298.png)

对于相关系数：

- ρ测量x和y之间的线性关系的强度和方向
- ρ=1，xy是一个线性增长的关系
- ρ=-1，xy是一个线性递减的关系
- ρ=0，xy是不相关的

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419192254208.png" alt="image-20230419192254208" style="zoom:67%;" />

​	协方差矩阵

给定随机向量，$𝐗=[x_1，x_2，…，x_N]^T$，我们定义，

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419192909565.png" alt="image-20230419192909565" style="zoom:80%;" />

协方差矩阵表示随机向量中每对维(特征)一起变化的趋势，即协变。

**重要的结论：**

- 如果$x_i$和$x_k$倾向于一起增加，那么$c_{ik}$>0

- 如果$x_i$在$x_k$增加时趋于减少，那么$c_{ik}$<0

- 如果$x_i$和$x_k$不相关，那么$c_{ik}$=0
- $|c_{ik}|$≤$σ_iσ_k$，其中$σ_i$为$x_i$的标准差

- $c_{ii}$=$σ_i^2$=VAR（$x_i$）

- 对称：$c_{ji}=c_{ij}$

- 半正定：

- 特征值是非负的

- 行列式是非负的|𝐶|≥0



### 特征值和特征向量、

定义：𝒗 是矩阵的特征向量𝑨 ∈ℝ^(𝑚∗𝑚) 如果存在标量𝜆, 使得：![image-20230419194118476](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419194118476.png)

v是矩阵的特征向量，λ是对应的特征值

![image-20230419194305725](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419194305725.png)

![image-20230419194355548](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419194355548.png)

积分：特征向量表示向量空间中不变的方向。

位于由定义的方向上的任何点𝒗 继续朝着这个方向前进。

其大小乘以相应的特征值𝜆

### 特征值分解

### 奇异值分解（SVD）（sigular value Decomposition）

在实践中，我们通过对中心数据矩阵的奇异值分解(SVD)来计算pc。

将一个非零的mxn矩阵A，表示维三个矩阵乘积的形式，即进行矩阵的因子分解

$A=U\sum V^T$

其中，$U$是m阶正交矩阵，$V$是n阶正交矩阵，$\sum$是由降序排列的非负的堆笑元素组成的mXn矩形对角矩阵

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419220141697.png" alt="image-20230419220141697" style="zoom:80%;" />

几何解释：线性变换𝑨 可以解释为三种几何变换的组合：旋转或反射(𝑽^𝑻), 然后是逐坐标缩放的坐标(𝚺), 接着是另一个旋转或反射(𝑼).

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419220326141.png" alt="image-20230419220326141" style="zoom:33%;" />

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419220507947.png" alt="image-20230419220507947" style="zoom:50%;" />

让我们把重点放在原始基向量的变换上𝑨

首先，通过旋转𝑽^𝑻, 我们有<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419220529112.png" alt="image-20230419220529112" style="zoom:50%;" />

第二，由𝚺, 我们按比例缩放$𝑽^𝑻 𝒆_𝟏 $和$𝑽^𝑻 𝒆_𝟐 $与原始坐标系一起$(𝒆_𝟏, 𝒆_𝟐)$<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419220725214.png" alt="image-20230419220725214" style="zoom:50%;" />

最后，通过旋转𝑼, 我们有<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419220710120.png" alt="image-20230419220710120" style="zoom:50%;" />

特殊性质：

U的列向量（即左奇异向量）是$𝑨𝑨^𝑻$的特征向量

V的列向量（即右奇异向量）是$𝑨^𝑻𝑨$的特征向量

$𝑨𝑨^𝑻$的特征值𝜆_1,…,𝜆_𝑟 是的$𝑨^𝑻 𝑨$的特征值

矩阵A的奇异值$𝜎_𝑖=√(𝜆_𝑖 )$

奇异值分解不要求矩阵A是方阵

证明V的行（即右奇异向量）是$𝑨^𝑻𝑨$的特征向量

#### 正交矩阵

正交矩阵是在欧几里得空间的叫法，一个正交矩阵对应的变换叫做正交变换，正交变换的特点是不改变向量的尺寸和向量间的夹角，

<img src="https://img-blog.csdn.net/20150123140255328" alt="20150123140255328 (247×66)" style="zoom:67%;" />

#### 紧奇异值分解（compact SVD)

![image-20230420091005638](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230420091005638.png)

#### 截断奇异值分解(truncated SVD)

![image-20230420091033738](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230420091033738.png)









## 05 LDA线性判别分析

监督学习算法，常用来做特征提取，数据降维，和任务分类

线性判别分析（Linear Discriminant Analysis）是一种**有监督学习的降维方法**，用于找到**分隔两个或多个对象类的特征的线性组合。**

也就是给定有标签的数据集，利用LDA实现一个较好的分类。谈及分类，那么就说一说特征提取： 特征提取（维数约简/特征约简）是指将原始高维数据映射到低维空间。而LDA就是给定数据集，将训练样本投影到一条直线上。使得类间距离最大化，同时使得类内距离最小化。

LDA算法与PCA算法都是常用的降维技术。二者的区别在于：LDA是一种监督学习的降维技术，也就是说它的每个样本是有类别输出的，而之前所学习的PCA算法是不考虑样本类别输出的无监督降维技术。

### 特征提取

特征提取（降维/特征降维）指的是将原始的高维的数据映射到低维的空间

特征提取的标准会根据问题设置而不同

无监督设置——最大限度的减少信息的损失

监督设置——最大限度的分类区别

### LDA

LDA是一种寻找特征线性组合的方法，可以将两个或者多个类别的对象分开，也叫隐含狄利克雷分布

有二分类和多分类

LDA算法的目标是使降维后的数据**类内方差最小，类间方差最大**

• 最大化类间距离Maximize the between-class distance (means)
• 最小化类内距离Minimize the within-class variability (scatter)

（希望投影后同一种数据的投影点尽可能接近，不同种数据中心之间的距离尽可能大）

![image-20230417212310380](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417212310380.png)

LDA的优化目标为最大类间方差和最小类内方差。LDA方法需分别计算“within-class”的分散程度Sw和“between-class”的分散程度Sb，而且希望Sb/Sw 越大越好，从而找到合适的映射向量w。

### LDA二分类

假设我们有一个d维的样本集，{x1,x2,x3,···，xd}，C1有n1个样本点，C2有n2个样本点，那么我们求一个转换，将d维投影到一条线上，实现降维。那么它的公式如下。

$y_i=θ^T x_i$,其中$x_i=[(x_i1⋮x_id )] and θ=[θ_1..θ_d )]$（这个是竖着的矩阵）

其中$\theta$是投影向量，将x投影到y上

**那么我们又怎么计算类内距离和类间距离呢？**

首先计算均值，每个类的x的均值和y的均值如下所示

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417213914545.png" alt="image-20230417213914545" style="zoom:67%;" />

将x投影到y会导致x的均值投影到y的均值

选择$\theta$以至于最大化投影均值的距离，![image-20230417214156263](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417214156263.png)

同时，最小化类内方差：

定义类内总方差，选择$\theta$来最小化方差

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417214338405.png" alt="image-20230417214338405" style="zoom:80%;" />

最终我们表达一个最终标准（如果$\theta$是一个答案，那么$\alpha\theta$也是一个答案）

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417214514678.png" alt="image-20230417214514678" style="zoom:67%;" />

λ是一个拉格朗日乘子，那么新的标准如下：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417214749959.png" alt="image-20230417214749959" style="zoom:67%;" />

对该函数求导；得到如下，令等式等于0,得到<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417214858733.png" alt="image-20230417214858733" style="zoom:67%;" />

$\theta$是$S_w^{-1} S_b$的特征向量，λ是对应的特征值<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417215224523.png" alt="image-20230417215224523" style="zoom:67%;" />

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417220805584.png" alt="image-20230417220805584" style="zoom:67%;" />

**LDA的流程：**

![image-20230418202818749](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230418202818749.png)

**流程2：**

1. 在训练集种建立X1和X2
2. 计算均值$μ_1$ and$ μ_2$
3. 计算$S_w$，<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417221842006.png" alt="image-20230417221842006" style="zoom:70%;" /><img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417221903965.png" alt="image-20230417221903965" style="zoom:70%;" /><img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417221917999.png" alt="image-20230417221917999" style="zoom:70%;" />
4. 计算<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417222019217.png" alt="image-20230417222019217" style="zoom:67%;" />
5. 计算<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417222102647.png" alt="image-20230417222102647" style="zoom:80%;" />

例题！！！二分类问题

![image-20230417222153043](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417222153043.png)

分别计算每一类的均值

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417222201307.png" alt="image-20230417222201307" style="zoom:80%;" />

第一个类别的协方差矩阵

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417222316835.png" alt="image-20230417222316835" style="zoom:80%;" />

第二个类别的协方差矩阵

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417222234779.png" alt="image-20230417222234779" style="zoom:80%;" />

类内散度矩阵

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417222339595.png" alt="image-20230417222339595" style="zoom:80%;" />

类间散度矩阵

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417222412016.png" alt="image-20230417222412016" style="zoom:80%;" />

计算特征值

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417222440155.png" alt="image-20230417222440155" style="zoom:80%;" />

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417222501640.png" alt="image-20230417222501640" style="zoom:80%;" />

最优投影就是最小化的投影

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417222516769.png" alt="image-20230417222516769" style="zoom:80%;" />

### LDA多分类

多分类与二分类不同的是类间散度举证和类内散度举证计算公式不同，其余根据概念计算即可

![image-20230418203445437](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230418203445437.png)

多分类过程：

![image-20230418203600186](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230418203600186.png)







## 06特征选择（feature selection）

### 监督学习（supervised learning)

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417210113276.png" alt="image-20230417210113276" style="zoom:50%;" />

数据+标签——>学习算法——>模型

训练数据——>模型——>预测标签

主要内容：

包装方法（wrappers methods)利用机器学习算法作为黑箱，找出特征的最佳子集

过滤方法（filter methods)在机器学习算法运行之前选择特征

嵌入式方法（embedded methods)作为机器学习算法的一部分，特征选择是自然发生的

过拟合/欠拟合

正则项

## 07贝叶斯分类器（bayes Classifiers)

在机器学习中，有监督分类的算法有很多，比如 SVM、ANN 以及基于决策树的 AdaBoost 和随机森林等。这其中自然也少不了今天的主角**朴素贝叶斯分类器（Naïve Bayes classifiers）**。

它代表着**一类**应用**贝叶斯定理**的分类器的总称。朴素（naive）在这里有着特殊的含义、代表着一个非常强的假设（下文会解释）。

### 贝叶斯定理

**条件概率可以定义为：在事件 B 发生的前提下，事件 A 发生的概率。**数学上用$P(A|B)$来表示该条件概率。

贝叶斯定理计算条件概率的公式来解决如下一类问题：

假设：
		H[1],H[2]…,H[n]互斥且构成一个完全事件
		已知它们的概率P(H[i]),i=1,2,…,n,
		现观察到某事件A与H[1],H[2]…,H[n]相伴随机出现且已知条件概率P(A|H[i])，求P(H[i]|A)

- A发生的概率记作：P(A)
- B发生的概率记作：P(B)
- A和B同时发生的概率记作：P(AB)
- 条件概率P(B|A)为：
  ![在这里插入图片描述](https://img-blog.csdnimg.cn/20191108152644537.png)

由上图公式可得：P(AB)=P(B|A)P(A)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20191108153638652.png)

### 贝叶斯决策规则

#### 先验分布和后验分布

贝叶斯统计中的两个基本概念是先验分布和后验分布：

- **先验分布（Prior）。**总体分布参数θ的一个概率分布。贝叶斯学派的根本观点，是认为在关于总体分布参数θ的任何统计推断问题中，除了使用样本所提供的信息外，还必须规定一个先验分布，它是在进行统计推断时不可缺少的一个要素。他们认为先验分布不必有客观的依据，可以部分地或完全地基于主观信念。

- **后验分布（Posterior）**。根据样本分布和未知参数的先验分布，用概率论中求条件概率分布的方法，求出的在样本已知下，未知参数的条件分布。因为这个分布是在抽样以后才得到的，故称为后验分布。贝叶斯推断方法的关键是任何推断都必须且只须根据后验分布，而不能再涉及样本分布。

- 先验概率$\sum_{i=1}^{M}{p(c_i)=1}$

- 证据因子$x:p(x)$

- 似然(likelihood)（类条件概率）$p(x|c_i)$

- 后验概率$x:p(c_i|x)$


$$p(c_i|x)=\frac{p(c_i)p(x|c_i)}{p(x)}  $$

$  后验=\frac{先验*似然}{证据因子}$

$$后验正比于先验*似然$$

考虑一个二分类问题：错误概率：

![image-20230417193304246](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417193304246.png)

平均错误概率(损失函数）：$p(e)=\int_{}{}{p(e|x)p(x)}$

最小化错误概率的贝叶斯决策规则：![image-20230417193433680](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417193433680.png)

最小错误概率：

![image-20230417193656082](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417193656082.png)

$X_B$：贝叶斯决策的决策边界，贝叶斯错误率的概率最小（阴影面积最小）即后验概率相等

**贝叶斯错误率**：一个分类问题可以达到的最小错误率

$x^*$非最优的决策点，粉色区域：性质为c2决定为c1的错误概率，灰色区域：相反

### 高斯分布（正态）

性质：中心极限定理——在适当的条件下，大量相互独立 随机变量 的均值经适当标准化后 依分布收敛 于标准 正态分布。

一元高斯分布：均值$\mu$和方差$\sigma$

![image-20230417194633056](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417194633056.png)

多元高斯分布：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417200119603.png" alt="image-20230417200119603" style="zoom:67%;" />

- $\mu$——均值向量

- $\sum$——协方差矩阵

- $|\sum|$——行列式

- $\sum^{-1}$——矩阵的逆


协方差矩阵

1. 对角线的元素是每个特征的方差

2. 任意俩个特征$x_i$和$x_j$之间的关系

   独立$\sigma_j$=0      正相关$\sigma_j$>0     负相关$\sigma_j$<0

$\sum$是对角矩阵

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417200737828.png" alt="image-20230417200737828" style="zoom:80%;" />

马氏距离和欧氏距离

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417201117196.png" alt="image-20230417201117196" style="zoom:80%;" />

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417201313658.png" alt="image-20230417201313658" style="zoom:67%;" />

### 高斯的判别函数

对于最小错误率：

$ g_i(x)=lnp(c_i|x)正比于Inp(x|c_i)+Inp(c_i)$

多元高斯的情况：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417201655478.png" alt="image-20230417201655478" style="zoom:67%;" />

#### **情况1：$Σ_i=σ^2 I$**，

是个对角矩阵，只有对角线有，而且相同，可以写成这种形式$Σ_i=σ^2 I$

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417202105088.png" alt="image-20230417202105088" style="zoom:80%;" />

线性判别函数：<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417202320226.png" alt="image-20230417202320226" style="zoom:67%;" />

一个用线性判别函数的分类器被称为“线性机器”，线性机的决策曲面是超平面的片段，由$g_i (x)=g_j (x)$

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230417202644633.png" alt="image-20230417202644633" style="zoom:80%;" />

两个性质——比较重要

**性质1：，正交！分隔R_i和R_j的超平面（直线）始终与连接均值（均值之间的连线）的线正交！**

**性质2：经过中心点，如果 $p（C_i ）=p（C_j ）$，则 $x_0=1/2（μ_i+μ_j）$,平均值 $1/2 （μ_i+μ_j ）$ 位于超平面上（即超平面穿过它）。**经过中心点

#### **情况2：$Σ_i=Σ$**

所有类的协方差相同但是任意

线性判别函数

两个性质

性质1：马氏空间垂直

性质2：经过中心点

#### 情况3：$Σ_i=σ^2 I$

每个类都有不同的协方差，与单位矩阵成比例，只有对角线有元素，但是每个元素不同

不是线性判别函数

#### **情况4：$\sum_i$=arbtrary**

每个类的协方差都不一样，不止对角线有元素，其他位置也有位置，每个元素都不同

不是线性判别函数

### 朴素贝叶斯分类器（Naive Bayes Classifier）

朴素贝叶斯分类器是一种基于贝叶斯定理(来自贝叶斯统计)的简单概率分类器，具有很强的(朴素)独立性假设。

独立特征模型

**假设**一个类的特定特征的存在与其他特征的存在无关（特征之间不相关）（独立）

尽管朴素的设计和过于简化的假设，朴素贝叶斯分类器再许多复杂的现实世界情况下工作任然**相当好**

（为什么朴素贝叶斯的假设这么强，效果还这么好？）

（因为数据降维后可能会导致强相关的变量变成弱相关性的变量，比如用PCA，LDA来降维，所以这种情况下，朴素贝叶斯的效果好）

它只需要少量的训练数据来估计分类所需的**参数（均值和方差）**

因为自变量是假设的，所以只需要每个类的变量的方差，**不需要整个协方差矩阵**（因为假设了每个变量都不相关）

### 朴素贝叶斯概率公式

分类器的概率模型是一个条件模型

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419162555795.png" alt="image-20230419162555795" style="zoom:70%;" />

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419162724455.png" alt="image-20230419162724455" style="zoom:80%;" />

其中C是分类，F1到Fd是特征变量，特征个数d,

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419162901833.png" alt="image-20230419162901833" style="zoom:80%;" />

在实际中，我们支队分子感兴趣，因为分母不依赖于c,并且特征F是给定的，所以分母事实上的常数

仔细观察分子

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419163306036.png" alt="image-20230419163306036" style="zoom:80%;" />

“朴素”假设：每个特征Fi是独立于Fj，所以<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419163423739.png" alt="image-20230419163423739" style="zoom:67%;" />

联合模型可以表示为：<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419163536468.png" alt="image-20230419163536468" style="zoom:80%;" />

所以，我们有<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419163711962.png" alt="image-20230419163711962" style="zoom:80%;" />

其中Z（证据）是依赖于F1...Fd的一个比例因子，也就是说，如果F1...Fd特征值是已知的，那么Z就是个常数。

所以这个模型就被分解为一个先验P(C)和一个概率分布P(Fi|C)

如果有𝑘 类，如果每个都有一个模型𝑝(𝐹𝑖|𝐶 = 𝑐) 可以用𝑟 参数，则相应的朴素贝叶斯模型具有(𝑘−1)+𝑑𝑟𝑘 参数

在实践中，𝑘=2经常（二进制分类）和𝑟=1（伯努利特征：𝑃_𝑟 (𝑋=1)=𝑝,𝑃_𝑟 (𝑋=0)=1−𝑝) 是常见的，因此朴素贝叶斯模型的参数总数为2𝑑+1，其中𝑑 是用于分类的特征数。

### 参数估计

我们可以用可能性的最大似然估计

给定一个数据集𝒟={𝒙_1.𝒙_2,…,𝒙_𝑛}, 其中𝑛 样本是从相同分布p中独立抽取的p(𝒙|𝜽), 估计参数𝜽.

ML估计参数𝜽 最大化p(𝒟|𝜽)

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419164628682.png" alt="image-20230419164628682" style="zoom:80%;" />

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419164648444.png" alt="image-20230419164648444" style="zoom:80%;" />

ML估计参数𝜽 最大化𝑝(𝑿,𝐶|𝜽), 𝑿=[𝐹_1.𝐹_2,…,𝐹_𝑑]<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419164815391.png" alt="image-20230419164815391" style="zoom:80%;" />

### 最大似然估计

所有模型参数（先验和类概率分布）都可以用训练集中的相对频率来近似，

让我们考虑二项式分布的MLE(𝑵 独立的伯努利试验），

允许𝑋~𝐵(𝑁,𝑝), 以及实际数据（成功次数）𝑋=𝑘, 那么它的可能性是

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419170024646.png" alt="image-20230419170024646" style="zoom:80%;" />

取的导数𝑙𝑛𝐿(𝑝|𝑘) 关于𝑝:<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419170103240.png" alt="image-20230419170103240" style="zoom:80%;" />我们有p=k/N





### 参数估计（最大似然估计）

允许𝑋~𝐵(𝑁,𝑝), 以及实际数据𝑋=𝑘, 然后基于MLE，我们𝑝=𝑘/𝑁.

类的先验可以估计为（给定类的先验）=（类中样本的数量）/（样本的总数），或者只是假设等概率类（即先验=1/（类的数量））。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419165751364.png" alt="image-20230419165751364" style="zoom:80%;" />

我们可以估计独立概率分布𝑝(𝐹_𝑗=𝑓_𝑗 |𝐶=𝑐) 如下所示：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419165838901.png" alt="image-20230419165838901" style="zoom:80%;" />

如果处理的是**连续数据**，一个典型的假设是，与每个类别相关的连续值是根据高斯分布分布的。

例如假设训练数据包含连续属性，𝑥. 我们首先按类划分数据，然后计算每个类的x的平均值$𝑢_𝑐$ 和方差$𝜎_𝑐^2$

让$𝑢_𝑐$ 是与类关联𝑐𝑥的平均值 , 和$𝜎_𝑐^2$是方差。

然后𝑃(𝑥=𝑣|𝑐) 可以通过堵塞来计算𝑣 代入如下方程：<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419171345207.png" alt="image-20230419171345207" style="zoom:80%;" />

从概率模型构造分类器

到目前为止的讨论已经导出了独立的特征模型，即朴素贝叶斯概率模型。

朴素贝叶斯分类器将该模型与决策规则相结合。

一个常见的规则是选择最有可能的假设；这被称为最大后验或MAP决策规则。

相应的分类器定义如下：<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419171510279.png" alt="image-20230419171510279" style="zoom:80%;" />

例题：

问题:根据测量的特征对给定的人是男性还是女性进行分类。这些特征包括身高、体重和脚的尺寸。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419171637791.png" alt="image-20230419171637791" style="zoom:67%;" />

使用高斯分布假设从训练集创建的分类器将是:<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419171716597.png" alt="image-20230419171716597" style="zoom:80%;" />

比方说𝑃(𝑚𝑎𝑙𝑒)= 𝑃(𝑓𝑒𝑚𝑎𝑙𝑒) = 0.5.

如果我们确定𝑃(𝐶) 根据训练集中的频率，我们碰巧得到了相同的答案。

将下面的测试样本分类为男性或女性。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419172154507.png" alt="image-20230419172154507" style="zoom:67%;" />

$posterior (male) = P(male)∗P(height|male)∗P(weight|male)∗P(foot size|male) / evidence $

$posterior (female) = P(female)∗P(height|female)∗P(weight|female)∗P(foot size|female) / evidence $

$evidence=P(male)∗P(height|male)∗P(weight|male)∗P(footsize|male)+P(female)∗P(height|female)∗P(weight|female)∗P(foot size|female) $

evidence可以忽略，因为它是一个正常数

![image-20230419172039066](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419172039066.png)

该样本为女性

### 文档分类

考虑垃圾邮件/非垃圾邮件的二进制文档分类。

我们可以用一组单词对文档进行建模，其中𝑖-第个单词出现在类中的文档中𝐶 可以写成$P(w_i|C)$

假设单词不依赖于文档的长度、文档中相对于其他单词的位置或其他文档上下文。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419172457582.png" alt="image-20230419172457582" style="zoom:67%;" />

给定文档𝐷 属于给定的类𝐶 的可能性 ,即𝑝(𝐶|𝐷)?

贝叶斯定理：$p(C|D)=\frac{p(C)}{p(D)}p(D|C)$

假设只有两个互斥的类，S和非S(例如:垃圾邮件/垃圾邮件),

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419172815380.png" alt="image-20230419172815380" style="zoom:100%;" />

概率比𝑝(𝑆|𝐷)/𝑝(¬𝑆|𝐷) 可以用一系列似然比来表示。

取所有这些比率的对数，我们得到![image-20230419173133168](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419173133168.png)

最后，文档可以被如下分类：

如果是垃圾邮件$𝑝(𝑆|𝐷)>𝑝(¬𝑆|𝐷) $（即$𝑙𝑛⁡((𝑝(𝑆|𝐷))/(𝑝(¬ 𝑆|𝐷)))>0$），否则它不是垃圾邮件。

### 样本矫正

拉普拉斯平滑（lasplace smoothing)

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419173535893.png" alt="image-20230419173535893" style="zoom:80%;" />

如果给定的类和特征值在训练集中从未同时出现，那么基于频率的概率估计将为零。

这是有问题的，因为当它们相乘时，它会消除其他概率中的所有信息。

因此，通常需要在所有概率估计中纳入小样本校正，这样就不会将任何概率设置为完全为零。

![image-20230419173605533](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230419173605533.png)

### 摘要

尽管影响深远的**独立性假设**通常是不准确的，但朴素贝叶斯分类器有几个适当的联系，这使得它在实践中非常有用。

类条件特征分布的解耦意味着**每个分布都可以独立地估计为一维分布**。这反过来又有助于缓解因维度诅咒而产生的问题。

与MAP决策规则下的所有概率分类器一样，只要正确的分类比任何其他类别的概率都大，它就会得到正确的分类;因此不需要很好地估计类概率

### 总结

- 朴素假设：每个特征都独立
- 后验=（先验*似然）/证据因子
- 最大似然估计：1.离散的特征 2. 连续的特征
- 样本矫正：拉普拉斯平滑

# 08 支持向量机SVM

支持向量机（support vector machines, SVM）是一种二分类模型，它的基本模型是定义在特征空间上的**间隔最大的线性分类器**，间隔最大使它有别于感知机；SVM还包括**核技巧**，这使它成为实质上的非线性分类器。SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。

### 线性可分支持向量机和硬间隔最大化

SVM学习的基本想法是求解能够正确划分训练数据集并且集合间隔最大的分离超平面（决策边界），如下图所示， ![ \boldsymbol{w}\cdot x+b=0 ](https://www.zhihu.com/equation?tex=+%5Cboldsymbol%7Bw%7D%5Ccdot+x%2Bb%3D0+&consumer=ZHI_MENG) 即为分离超平面，对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机)，但是几何间隔最大的分离超平面却是唯一的。

<img src="https://pic1.zhimg.com/v2-197913c461c1953c30b804b4a7eddfcc_b.webp?consumer=ZHI_MENG" alt="img" style="zoom:80%;" />

学习的目的是在特征空间中找到一个分离超平面。

**hyperplane超平面**

- 将n维空间分成两个半空间
- 定义一个指向外的法向量
- 假设：超平面通过原点，如果不是：
- 有一个偏向项b
- b>0意味着沿着w方向平移它

在推导之前，先给出一些定义。假设给定一个特征空间上的训练数据集

![ T=\left\{ \left( \boldsymbol{x}_1,y_1 \right) ,\left( \boldsymbol{x}_2,y_2 \right) ,...,\left( \boldsymbol{x}_N,y_N \right) \right\} ](https://www.zhihu.com/equation?tex=+T%3D%5Cleft%5C%7B+%5Cleft%28+%5Cboldsymbol%7Bx%7D_1%2Cy_1+%5Cright%29+%2C%5Cleft%28+%5Cboldsymbol%7Bx%7D_2%2Cy_2+%5Cright%29+%2C...%2C%5Cleft%28+%5Cboldsymbol%7Bx%7D_N%2Cy_N+%5Cright%29+%5Cright%5C%7D+&consumer=ZHI_MENG)

其中， ![\boldsymbol{x}_i\in \mathbb{R}^n ](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bx%7D_i%5Cin+%5Cmathbb%7BR%7D%5En+&consumer=ZHI_MENG) ， ![ y_i\in \left\{ +1,-1 \right\} ,i=1,2,...N ](https://www.zhihu.com/equation?tex=+y_i%5Cin+%5Cleft%5C%7B+%2B1%2C-1+%5Cright%5C%7D+%2Ci%3D1%2C2%2C...N+&consumer=ZHI_MENG) ， ![x_i ](https://www.zhihu.com/equation?tex=x_i+&consumer=ZHI_MENG) 为第 ![ i ](https://www.zhihu.com/equation?tex=+i+&consumer=ZHI_MENG) 个特征向量， ![ y_i ](https://www.zhihu.com/equation?tex=+y_i+&consumer=ZHI_MENG) 为类标记，当它等于+1时为正例；为-1时为负例。再假设训练数据集是**线性可分**的。

**几何间隔**

对于给定的数据集 ![ T ](https://www.zhihu.com/equation?tex=+T+&consumer=ZHI_MENG) 和超平面 ![w\cdot x+b=0](https://www.zhihu.com/equation?tex=w%5Ccdot+x%2Bb%3D0&consumer=ZHI_MENG) ，定义超平面关于样本点 ![ \left( x_i,y_i \right) ](https://www.zhihu.com/equation?tex=+%5Cleft%28+x_i%2Cy_i+%5Cright%29+&consumer=ZHI_MENG) 的几何间隔为

<img src="https://www.zhihu.com/equation?tex=+%5Cgamma+_i%3Dy_i%5Cleft%28+%5Cfrac%7B%5Cboldsymbol%7Bw%7D%7D%7B%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2B%5Cfrac%7Bb%7D%7B%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert%7D+%5Cright%29+&consumer=ZHI_MENG" alt=" \gamma _i=y_i\left( \frac{\boldsymbol{w}}{\lVert \boldsymbol{w} \rVert}\cdot \boldsymbol{x}_{\boldsymbol{i}}+\frac{b}{\lVert \boldsymbol{w} \rVert} \right) " style="zoom:67%;" />

超平面关于所有样本点的几何间隔的最小值为

![ \gamma =\underset{i=1,2...,N}{\min}\gamma _i ](https://www.zhihu.com/equation?tex=+%5Cgamma+%3D%5Cunderset%7Bi%3D1%2C2...%2CN%7D%7B%5Cmin%7D%5Cgamma+_i+&consumer=ZHI_MENG)

最大化几何间隔：

![image-20230424193946013](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230424193946013.png)

实际上这个距离就是我们所谓的**支持向量**到超平面的距离。

因此，SVM模型的求解最大分割超平面问题可以表示为以下约束最优化问题

![ \underset{\boldsymbol{w,}b}{\max}\ \gamma ](https://www.zhihu.com/equation?tex=+%5Cunderset%7B%5Cboldsymbol%7Bw%2C%7Db%7D%7B%5Cmax%7D%5C+%5Cgamma+&consumer=ZHI_MENG)

![ s.t.\ \ \ y_i\left( \frac{\boldsymbol{w}}{\lVert \boldsymbol{w} \rVert}\cdot \boldsymbol{x}_{\boldsymbol{i}}+\frac{b}{\lVert \boldsymbol{w} \rVert} \right) \ge \gamma \ ,i=1,2,...,N ](https://www.zhihu.com/equation?tex=+s.t.%5C+%5C+%5C+y_i%5Cleft%28+%5Cfrac%7B%5Cboldsymbol%7Bw%7D%7D%7B%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2B%5Cfrac%7Bb%7D%7B%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert%7D+%5Cright%29+%5Cge+%5Cgamma+%5C+%2Ci%3D1%2C2%2C...%2CN+&consumer=ZHI_MENG)

将约束条件两边同时除以 ![ \gamma ](https://www.zhihu.com/equation?tex=+%5Cgamma+&consumer=ZHI_MENG) ，得到

![ y_i\left( \frac{\boldsymbol{w}}{\lVert \boldsymbol{w} \rVert \gamma}\cdot \boldsymbol{x}_{\boldsymbol{i}}+\frac{b}{\lVert \boldsymbol{w} \rVert \gamma} \right) \ge 1 ](https://www.zhihu.com/equation?tex=+y_i%5Cleft%28+%5Cfrac%7B%5Cboldsymbol%7Bw%7D%7D%7B%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert+%5Cgamma%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2B%5Cfrac%7Bb%7D%7B%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert+%5Cgamma%7D+%5Cright%29+%5Cge+1+&consumer=ZHI_MENG)

因为 ![ \lVert \boldsymbol{w} \rVert \text{，}\gamma ](https://www.zhihu.com/equation?tex=+%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert+%5Ctext%7B%EF%BC%8C%7D%5Cgamma+&consumer=ZHI_MENG) 都是标量，所以为了表达式简洁起见，令

![\boldsymbol{w}=\frac{\boldsymbol{w}}{\lVert \boldsymbol{w} \rVert \gamma}](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bw%7D%3D%5Cfrac%7B%5Cboldsymbol%7Bw%7D%7D%7B%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert+%5Cgamma%7D&consumer=ZHI_MENG)

![b=\frac{b}{\lVert \boldsymbol{w} \rVert \gamma} ](https://www.zhihu.com/equation?tex=b%3D%5Cfrac%7Bb%7D%7B%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert+%5Cgamma%7D+&consumer=ZHI_MENG)

得到

![y_i\left( \boldsymbol{w}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) \ge 1,\ i=1,2,...,N](https://www.zhihu.com/equation?tex=y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+%5Cge+1%2C%5C+i%3D1%2C2%2C...%2CN&consumer=ZHI_MENG)

又因为最大化 ![ \gamma ](https://www.zhihu.com/equation?tex=+%5Cgamma+&consumer=ZHI_MENG) ，等价于最大化 ![ \frac{1}{\lVert \boldsymbol{w} \rVert}](https://www.zhihu.com/equation?tex=+%5Cfrac%7B1%7D%7B%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert%7D&consumer=ZHI_MENG) ，也就等价于最小化 ![ \frac{1}{2}\lVert \boldsymbol{w} \rVert ^2 ](https://www.zhihu.com/equation?tex=+%5Cfrac%7B1%7D%7B2%7D%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert+%5E2+&consumer=ZHI_MENG) （ ![\frac{1}{2}](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D&consumer=ZHI_MENG) 是为了后面求导以后形式简洁，不影响结果），因此SVM模型的求解最大分割超平面问题又可以表示为以下约束最优化问题

![ \underset{\boldsymbol{w,}b}{\min}\ \frac{1}{2}\lVert \boldsymbol{w} \rVert ^2 ](https://www.zhihu.com/equation?tex=+%5Cunderset%7B%5Cboldsymbol%7Bw%2C%7Db%7D%7B%5Cmin%7D%5C+%5Cfrac%7B1%7D%7B2%7D%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert+%5E2+&consumer=ZHI_MENG)

![ s.t.\ \ y_i\left( \boldsymbol{w}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) \ge 1,\ i=1,2,...,N ](https://www.zhihu.com/equation?tex=+s.t.%5C+%5C+y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+%5Cge+1%2C%5C+i%3D1%2C2%2C...%2CN+&consumer=ZHI_MENG)

这是一个**含有不等式约束的凸二次规划问题**，可以对其使用拉格朗日乘子法得到其对偶问题（dual problem）。

举个例子：表示上述的优化问题

![image-20230424194401162](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230424194401162.png)

该优化问题可以用商业二次规划QP代码求解

然而这里使用拉格朗日对偶来解决

![image-20230424202345830](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230424202345830.png)

![image-20230424202424481](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230424202424481.png)

![image-20230424202442542](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230424202442542.png)



首先，我们将有约束的原始目标函数转换为**无约束的新构造的拉格朗日目标函数**

![L\left( \boldsymbol{w,}b,\boldsymbol{\alpha } \right) =\frac{1}{2}\lVert \boldsymbol{w} \rVert ^2-\sum_{i=1}^N{\alpha _i\left( y_i\left( \boldsymbol{w}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) -1 \right)} ](https://www.zhihu.com/equation?tex=L%5Cleft%28+%5Cboldsymbol%7Bw%2C%7Db%2C%5Cboldsymbol%7B%5Calpha+%7D+%5Cright%29+%3D%5Cfrac%7B1%7D%7B2%7D%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert+%5E2-%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_i%5Cleft%28+y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+-1+%5Cright%29%7D+&consumer=ZHI_MENG)

其中 ![ \alpha _i ](https://www.zhihu.com/equation?tex=+%5Calpha+_i+&consumer=ZHI_MENG) 为拉格朗日乘子，且 ![ \alpha _i\ge 0 ](https://www.zhihu.com/equation?tex=+%5Calpha+_i%5Cge+0+&consumer=ZHI_MENG) 。现在我们令

![ \theta \left( \boldsymbol{w} \right) =\underset{\alpha _{_i}\ge 0}{\max}\ L\left( \boldsymbol{w,}b,\boldsymbol{\alpha } \right) ](https://www.zhihu.com/equation?tex=+%5Ctheta+%5Cleft%28+%5Cboldsymbol%7Bw%7D+%5Cright%29+%3D%5Cunderset%7B%5Calpha+_%7B_i%7D%5Cge+0%7D%7B%5Cmax%7D%5C+L%5Cleft%28+%5Cboldsymbol%7Bw%2C%7Db%2C%5Cboldsymbol%7B%5Calpha+%7D+%5Cright%29+&consumer=ZHI_MENG)

$$L(w,\alpha,\beta)=f(w)+\sum{a_ig_i(w)+\sum{\beta_ih_i(w)}}$$

$\theta_p(w)=maxL(w,\alpha,\beta)$

$\theta(w)=max{f(x)+\sum{a_ig_i(w)}+\sum\beta_ih_i(w)}$

当样本点不满足约束条件时，即在可行解区域外：

![ y_i\left( \boldsymbol{w}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) <1 ](https://www.zhihu.com/equation?tex=+y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+%3C1+&consumer=ZHI_MENG)

此时，将 ![\alpha _i ](https://www.zhihu.com/equation?tex=%5Calpha+_i+&consumer=ZHI_MENG) 设置为无穷大，则 ![ \theta \left( \boldsymbol{w} \right) ](https://www.zhihu.com/equation?tex=+%5Ctheta+%5Cleft%28+%5Cboldsymbol%7Bw%7D+%5Cright%29+&consumer=ZHI_MENG) 也为无穷大。

当满本点满足约束条件时，即在可行解区域内：

![y_i\left( \boldsymbol{w}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) \ge 1 ](https://www.zhihu.com/equation?tex=y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+%5Cge+1+&consumer=ZHI_MENG)

此时， ![ \theta \left( \boldsymbol{w} \right) ](https://www.zhihu.com/equation?tex=+%5Ctheta+%5Cleft%28+%5Cboldsymbol%7Bw%7D+%5Cright%29+&consumer=ZHI_MENG) 为原函数本身。

$$\theta(w)=f(w)=\frac{1}{2}||w^2||$$

于是，将两种情况合并起来就可以得到我们新的目标函数

![ \theta \left( \boldsymbol{w} \right) =\begin{cases} \frac{1}{2}\lVert \boldsymbol{w} \rVert ^2\ ,\boldsymbol{x}\in \text{可行区域}\\ +\infty \ \ \ \ \ ,\boldsymbol{x}\in \text{不可行区域}\\ \end{cases} ](https://www.zhihu.com/equation?tex=+%5Ctheta+%5Cleft%28+%5Cboldsymbol%7Bw%7D+%5Cright%29+%3D%5Cbegin%7Bcases%7D+%5Cfrac%7B1%7D%7B2%7D%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert+%5E2%5C+%2C%5Cboldsymbol%7Bx%7D%5Cin+%5Ctext%7B%E5%8F%AF%E8%A1%8C%E5%8C%BA%E5%9F%9F%7D%5C%5C+%2B%5Cinfty+%5C+%5C+%5C+%5C+%5C+%2C%5Cboldsymbol%7Bx%7D%5Cin+%5Ctext%7B%E4%B8%8D%E5%8F%AF%E8%A1%8C%E5%8C%BA%E5%9F%9F%7D%5C%5C+%5Cend%7Bcases%7D+&consumer=ZHI_MENG)

于是原约束问题就等价于

![ \underset{\boldsymbol{w,}b}{\min}\ \theta \left( \boldsymbol{w} \right) =\underset{\boldsymbol{w,}b}{\min}\underset{\alpha _i\ge 0}{\max}\ L\left( \boldsymbol{w,}b,\boldsymbol{\alpha } \right) =p^* ](https://www.zhihu.com/equation?tex=+%5Cunderset%7B%5Cboldsymbol%7Bw%2C%7Db%7D%7B%5Cmin%7D%5C+%5Ctheta+%5Cleft%28+%5Cboldsymbol%7Bw%7D+%5Cright%29+%3D%5Cunderset%7B%5Cboldsymbol%7Bw%2C%7Db%7D%7B%5Cmin%7D%5Cunderset%7B%5Calpha+_i%5Cge+0%7D%7B%5Cmax%7D%5C+L%5Cleft%28+%5Cboldsymbol%7Bw%2C%7Db%2C%5Cboldsymbol%7B%5Calpha+%7D+%5Cright%29+%3Dp%5E%2A+&consumer=ZHI_MENG)

看一下我们的新目标函数，先求最大值，再求最小值。这样的话，我们首先就要面对带有需要求解的参数 ![ \boldsymbol{w} ](https://www.zhihu.com/equation?tex=+%5Cboldsymbol%7Bw%7D+&consumer=ZHI_MENG) 和 ![ b](https://www.zhihu.com/equation?tex=+b&consumer=ZHI_MENG) 的方程，而 ![ \alpha _i ](https://www.zhihu.com/equation?tex=+%5Calpha+_i+&consumer=ZHI_MENG) 又是不等式约束，这个求解过程不好做。所以，我们需要使用拉格朗日函数**对偶性**，将最小和最大的位置交换一下，这样就变成了：

![ \underset{\alpha _i\ge 0}{\max}\underset{\boldsymbol{w,}b}{\min}\ L\left( \boldsymbol{w,}b,\boldsymbol{\alpha } \right) =d^* ](https://www.zhihu.com/equation?tex=+%5Cunderset%7B%5Calpha+_i%5Cge+0%7D%7B%5Cmax%7D%5Cunderset%7B%5Cboldsymbol%7Bw%2C%7Db%7D%7B%5Cmin%7D%5C+L%5Cleft%28+%5Cboldsymbol%7Bw%2C%7Db%2C%5Cboldsymbol%7B%5Calpha+%7D+%5Cright%29+%3Dd%5E%2A+&consumer=ZHI_MENG)

$d^*=max_{\alpha,\beta,\alpha_i>=0}  min_{w}L(w,\alpha,\beta)<=min_{w} min_{\alpha,\beta,\alpha_i>=0}{L(w,\alpha,\beta)=p^*}$

最大最小值总是小于或等于最小最大值。

要有 ![ p^*=d^* ](https://www.zhihu.com/equation?tex=+p%5E%2A%3Dd%5E%2A+&consumer=ZHI_MENG) ，需要满足两个条件：

① 优化问题是凸优化问题

② 满足KKT条件

**在KKT条件下，可以得到$d^*=p^*$.**

当强对偶（$d^*=p^*$),我们可以**通过解决对偶问题来解决原始问题**

首先，本优化问题显然是一个凸优化问题，所以条件一满足，而要满足条件二，即要求

![ \begin{cases} \alpha _i\ge 0\\ y_i\left( \boldsymbol{w}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) -1\ge 0\\ \alpha _i\left( y_i\left( \boldsymbol{w}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) -1 \right) =0\\ \end{cases} ](https://www.zhihu.com/equation?tex=+%5Cbegin%7Bcases%7D+%5Calpha+_i%5Cge+0%5C%5C+y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D_%7B%5Cboldsymbol%7Bi%7D%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+-1%5Cge+0%5C%5C+%5Calpha+_i%5Cleft%28+y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D_%7B%5Cboldsymbol%7Bi%7D%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+-1+%5Cright%29+%3D0%5C%5C+%5Cend%7Bcases%7D+&consumer=ZHI_MENG)

为了得到求解对偶问题的具体形式，令 ![ L\left( \boldsymbol{w,}b,\boldsymbol{\alpha } \right) ](https://www.zhihu.com/equation?tex=+L%5Cleft%28+%5Cboldsymbol%7Bw%2C%7Db%2C%5Cboldsymbol%7B%5Calpha+%7D+%5Cright%29+&consumer=ZHI_MENG) 对 ![ \boldsymbol{w}](https://www.zhihu.com/equation?tex=+%5Cboldsymbol%7Bw%7D&consumer=ZHI_MENG) 和 ![ b ](https://www.zhihu.com/equation?tex=+b+&consumer=ZHI_MENG) 的偏导为0，可得

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230426162056401.png" alt="image-20230426162056401" style="zoom:50%;" />

对 ![ \boldsymbol{w}](https://www.zhihu.com/equation?tex=+%5Cboldsymbol%7Bw%7D&consumer=ZHI_MENG) 和 ![ b ](https://www.zhihu.com/equation?tex=+b+&consumer=ZHI_MENG) 的偏导

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230426161953007.png" alt="image-20230426161953007" style="zoom:50%;" />

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230426162006892.png" alt="image-20230426162006892" style="zoom:50%;" />

得到w，

![\boldsymbol{w}=\sum_{i=1}^N{\alpha _iy_i\boldsymbol{x}_{\boldsymbol{i}}} ](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bw%7D%3D%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_iy_i%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%7D+&consumer=ZHI_MENG)

![\sum_{i=1}^N{\alpha _iy_i}=0 ](https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_iy_i%7D%3D0+&consumer=ZHI_MENG)

将以上两个等式**带入拉格朗日目标函数（$L(w,b,\alpha)$)**，消去 ![ \boldsymbol{w}](https://www.zhihu.com/equation?tex=+%5Cboldsymbol%7Bw%7D&consumer=ZHI_MENG) 和 ![ b ](https://www.zhihu.com/equation?tex=+b+&consumer=ZHI_MENG) ， 得

![ L\left( \boldsymbol{w,}b,\boldsymbol{\alpha } \right) =\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{\alpha _i\alpha _jy_iy_j\left( \boldsymbol{x}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{j}} \right)}}-\sum_{i=1}^N{\alpha _iy_i\left( \left( \sum_{j=1}^N{\alpha _jy_j\boldsymbol{x}_{\boldsymbol{j}}} \right) \cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) +}\sum_{i=1}^N{\alpha _i} ](https://www.zhihu.com/equation?tex=+L%5Cleft%28+%5Cboldsymbol%7Bw%2C%7Db%2C%5Cboldsymbol%7B%5Calpha+%7D+%5Cright%29+%3D%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%3D1%7D%5EN%7B%5Csum_%7Bj%3D1%7D%5EN%7B%5Calpha+_i%5Calpha+_jy_iy_j%5Cleft%28+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bj%7D%7D+%5Cright%29%7D%7D-%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_iy_i%5Cleft%28+%5Cleft%28+%5Csum_%7Bj%3D1%7D%5EN%7B%5Calpha+_jy_j%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bj%7D%7D%7D+%5Cright%29+%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+%2B%7D%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_i%7D+&consumer=ZHI_MENG)

![image-20230424195808561](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230424195808561.png)

即,lost function新的目标函数，自变量是$\alpha$的一个二次函数
![\underset{\boldsymbol{w,}b}{\min}\ L\left( \boldsymbol{w,}b,\boldsymbol{\alpha } \right) =-\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{\alpha _i\alpha _jy_iy_j\left( \boldsymbol{x}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{j}} \right)}}+\sum_{i=1}^N{\alpha _i} ](https://www.zhihu.com/equation?tex=%5Cunderset%7B%5Cboldsymbol%7Bw%2C%7Db%7D%7B%5Cmin%7D%5C+L%5Cleft%28+%5Cboldsymbol%7Bw%2C%7Db%2C%5Cboldsymbol%7B%5Calpha+%7D+%5Cright%29+%3D-%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%3D1%7D%5EN%7B%5Csum_%7Bj%3D1%7D%5EN%7B%5Calpha+_i%5Calpha+_jy_iy_j%5Cleft%28+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bj%7D%7D+%5Cright%29%7D%7D%2B%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_i%7D+&consumer=ZHI_MENG)

现在问题变成了
<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230426162948653.png" alt="image-20230426162948653" style="zoom:50%;" />

求 ![ \underset{\boldsymbol{w,}b}{\min}\ L\left( \boldsymbol{w,}b,\boldsymbol{\alpha } \right) ](https://www.zhihu.com/equation?tex=+%5Cunderset%7B%5Cboldsymbol%7Bw%2C%7Db%7D%7B%5Cmin%7D%5C+L%5Cleft%28+%5Cboldsymbol%7Bw%2C%7Db%2C%5Cboldsymbol%7B%5Calpha+%7D+%5Cright%29+&consumer=ZHI_MENG) 对 ![ \boldsymbol{\alpha } ](https://www.zhihu.com/equation?tex=+%5Cboldsymbol%7B%5Calpha+%7D+&consumer=ZHI_MENG) 的极大，即是对偶问题

![\underset{\boldsymbol{\alpha }}{\max}\ -\frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{\alpha _i\alpha _jy_iy_j\left( \boldsymbol{x}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{j}} \right)}}+\sum_{i=1}^N{\alpha _i} ](https://www.zhihu.com/equation?tex=%5Cunderset%7B%5Cboldsymbol%7B%5Calpha+%7D%7D%7B%5Cmax%7D%5C+-%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%3D1%7D%5EN%7B%5Csum_%7Bj%3D1%7D%5EN%7B%5Calpha+_i%5Calpha+_jy_iy_j%5Cleft%28+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bj%7D%7D+%5Cright%29%7D%7D%2B%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_i%7D+&consumer=ZHI_MENG)

![ s.t.\ \ \ \ \sum_{i=1}^N{\alpha _iy_i}=0 ](https://www.zhihu.com/equation?tex=+s.t.%5C+%5C+%5C+%5C+%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_iy_i%7D%3D0+&consumer=ZHI_MENG)

![ \ \ \ \ \ \ \ \alpha _i\ge 0,\ i=1,2,...,N](https://www.zhihu.com/equation?tex=+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5Calpha+_i%5Cge+0%2C%5C+i%3D1%2C2%2C...%2CN&consumer=ZHI_MENG)

把目标式子加一个负号，将求解极大转换为求解极小

![ \underset{\boldsymbol{\alpha }}{\min}\ \frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{\alpha _i\alpha _jy_iy_j\left( \boldsymbol{x}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{j}} \right)}}-\sum_{i=1}^N{\alpha _i} ](https://www.zhihu.com/equation?tex=+%5Cunderset%7B%5Cboldsymbol%7B%5Calpha+%7D%7D%7B%5Cmin%7D%5C+%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%3D1%7D%5EN%7B%5Csum_%7Bj%3D1%7D%5EN%7B%5Calpha+_i%5Calpha+_jy_iy_j%5Cleft%28+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bj%7D%7D+%5Cright%29%7D%7D-%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_i%7D+&consumer=ZHI_MENG)

![ s.t.\ \ \ \ \sum_{i=1}^N{\alpha _iy_i}=0 ](https://www.zhihu.com/equation?tex=+s.t.%5C+%5C+%5C+%5C+%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_iy_i%7D%3D0+&consumer=ZHI_MENG)

![\ \ \ \ \ \ \ \alpha _i\ge 0,\ i=1,2,...,N ](https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5Calpha+_i%5Cge+0%2C%5C+i%3D1%2C2%2C...%2CN+&consumer=ZHI_MENG)

现在我们的**优化问题变成了如上的形式**。

例题：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230426163145701.png" alt="image-20230426163145701" style="zoom:67%;" />

对于对偶优化问题，我们可以用坐标上升法来解决：

考虑尝试解决无约束的优化问题：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230426163942292.png" alt="image-20230426163942292" style="zoom:33%;" />

**坐标上升法**（Coordinate Ascent)

算法：

循环直到收敛{

​	for i=1,...l{

​		$\alpha_i:=argmax_{\alpha_i}L(\alpha_1,...\alpha_{i-1},\alpha_i,\alpha_{i+1},...\alpha_l)$

​	}

}



**SMO算法 序列最小最优化sequential minimal Optimization**

[(71条消息) 序列最小最优化算法（SMO算法）_是帆帆不是凡凡呀的博客-CSDN博客](https://blog.csdn.net/m0_61624005/article/details/122218831)



<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230426165755314.png" alt="image-20230426165755314" style="zoom:50%;" />

我们通过这个优化算法能得到 ![\boldsymbol{\alpha }^* ](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7B%5Calpha+%7D%5E%2A+&consumer=ZHI_MENG) ，再根据 ![\boldsymbol{\alpha }^* ](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7B%5Calpha+%7D%5E%2A+&consumer=ZHI_MENG) ，我们就可以求解出 ![ \boldsymbol{w}](https://www.zhihu.com/equation?tex=+%5Cboldsymbol%7Bw%7D&consumer=ZHI_MENG) 和 ![ b ](https://www.zhihu.com/equation?tex=+b+&consumer=ZHI_MENG) ，进而求得我们最初的目的：找到超平面，即”决策平面”。

前面的推导都是假设满足KKT条件下成立的，KKT条件如下

![ \begin{cases} \alpha _i\ge 0\\ y_i\left( \boldsymbol{w}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) -1\ge 0\\ \alpha _i\left( y_i\left( \boldsymbol{w}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) -1 \right) =0\\ \end{cases} ](https://www.zhihu.com/equation?tex=+%5Cbegin%7Bcases%7D+%5Calpha+_i%5Cge+0%5C%5C+y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D_%7B%5Cboldsymbol%7Bi%7D%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+-1%5Cge+0%5C%5C+%5Calpha+_i%5Cleft%28+y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D_%7B%5Cboldsymbol%7Bi%7D%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+-1+%5Cright%29+%3D0%5C%5C+%5Cend%7Bcases%7D+&consumer=ZHI_MENG)

另外，根据前面的推导，还有下面两个式子成立

![\boldsymbol{w}=\sum_{i=1}^N{\alpha _iy_i\boldsymbol{x}_{\boldsymbol{i}}} ](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bw%7D%3D%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_iy_i%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%7D+&consumer=ZHI_MENG)

![\sum_{i=1}^N{\alpha _iy_i}=0 ](https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_iy_i%7D%3D0+&consumer=ZHI_MENG)

由此可知在 ![\boldsymbol{\alpha }^* ](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7B%5Calpha+%7D%5E%2A+&consumer=ZHI_MENG) 中，至少存在一个 ![ \alpha _{j}^{*}>0](https://www.zhihu.com/equation?tex=+%5Calpha+_%7Bj%7D%5E%7B%2A%7D%3E0&consumer=ZHI_MENG) （反证法可以证明，若全为0，则 ![ \boldsymbol{w}=0 ](https://www.zhihu.com/equation?tex=+%5Cboldsymbol%7Bw%7D%3D0+&consumer=ZHI_MENG) ，矛盾），对此 ![ j ](https://www.zhihu.com/equation?tex=+j+&consumer=ZHI_MENG) 有

![ y_j\left( \boldsymbol{w}^*\cdot \boldsymbol{x}_{\boldsymbol{j}}+b^* \right) -1=0 ](https://www.zhihu.com/equation?tex=+y_j%5Cleft%28+%5Cboldsymbol%7Bw%7D%5E%2A%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bj%7D%7D%2Bb%5E%2A+%5Cright%29+-1%3D0+&consumer=ZHI_MENG)

因此可以得到

![\boldsymbol{w}^*=\sum_{i=1}^N{\alpha _{i}^{*}y_i\boldsymbol{x}_i} ](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bw%7D%5E%2A%3D%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_%7Bi%7D%5E%7B%2A%7Dy_i%5Cboldsymbol%7Bx%7D_i%7D+&consumer=ZHI_MENG)

![ b^*=y_j-\sum_{i=1}^N{\alpha _{i}^{*}y_i\left( \boldsymbol{x}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{j}} \right)} ](https://www.zhihu.com/equation?tex=+b%5E%2A%3Dy_j-%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_%7Bi%7D%5E%7B%2A%7Dy_i%5Cleft%28+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bj%7D%7D+%5Cright%29%7D+&consumer=ZHI_MENG)

对于任意训练样本 ![ \left( \boldsymbol{x}_{\boldsymbol{i}},y_i \right) ](https://www.zhihu.com/equation?tex=+%5Cleft%28+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Cy_i+%5Cright%29+&consumer=ZHI_MENG) ，总有 ![ \alpha _i=0 ](https://www.zhihu.com/equation?tex=+%5Calpha+_i%3D0+&consumer=ZHI_MENG) 或者 ![y_{j}\left(\boldsymbol{w} \cdot \boldsymbol{x}_{j}+b\right)=1](https://www.zhihu.com/equation?tex=y_%7Bj%7D%5Cleft%28%5Cboldsymbol%7Bw%7D+%5Ccdot+%5Cboldsymbol%7Bx%7D_%7Bj%7D%2Bb%5Cright%29%3D1&consumer=ZHI_MENG) 。若 ![ \alpha _i=0 ](https://www.zhihu.com/equation?tex=+%5Calpha+_i%3D0+&consumer=ZHI_MENG) ，则该样本不会在最后求解模型参数的式子中出现。若 ![ \alpha _i>0](https://www.zhihu.com/equation?tex=+%5Calpha+_i%3E0&consumer=ZHI_MENG) ，则必有 ![ y_j\left( \boldsymbol{w}\cdot \boldsymbol{x}_{\boldsymbol{j}}+b \right) =1 ](https://www.zhihu.com/equation?tex=+y_j%5Cleft%28+%5Cboldsymbol%7Bw%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bj%7D%7D%2Bb+%5Cright%29+%3D1+&consumer=ZHI_MENG) ，所对应的样本点位于最大间隔边界上，是一个支持向量。这显示出支持向量机的一个重要性质：**训练完成后，大部分的训练样本都不需要保留，最终模型仅与支持向量（support vector)有关。**

到这里都是基于训练集数据线性可分的假设下进行的，但是实际情况下几乎不存在完全线性可分的数据，为了解决这个问题，引入了“软间隔”的概念，即允许某些点不满足约束

![y_j\left( \boldsymbol{w}\cdot \boldsymbol{x}_{\boldsymbol{j}}+b \right) \ge 1](https://www.zhihu.com/equation?tex=y_j%5Cleft%28+%5Cboldsymbol%7Bw%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bj%7D%7D%2Bb+%5Cright%29+%5Cge+1&consumer=ZHI_MENG)

采用hinge损失，将原优化问题改写为

![\underset{\boldsymbol{w,}b,\xi _i}{\min}\ \frac{1}{2}\lVert \boldsymbol{w} \rVert ^2+C\sum_{i=1}^m{\xi _i} ](https://www.zhihu.com/equation?tex=%5Cunderset%7B%5Cboldsymbol%7Bw%2C%7Db%2C%5Cxi+_i%7D%7B%5Cmin%7D%5C+%5Cfrac%7B1%7D%7B2%7D%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert+%5E2%2BC%5Csum_%7Bi%3D1%7D%5Em%7B%5Cxi+_i%7D+&consumer=ZHI_MENG)

![ s.t.\ \ y_i\left( \boldsymbol{w}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) \ge 1-\xi _i ](https://www.zhihu.com/equation?tex=+s.t.%5C+%5C+y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+%5Cge+1-%5Cxi+_i+&consumer=ZHI_MENG)

![ \ \ \ \ \ \xi _i\ge 0\ ,\ i=1,2,...,N ](https://www.zhihu.com/equation?tex=+%5C+%5C+%5C+%5C+%5C+%5Cxi+_i%5Cge+0%5C+%2C%5C+i%3D1%2C2%2C...%2CN+&consumer=ZHI_MENG)

其中 ![ \xi _i ](https://www.zhihu.com/equation?tex=+%5Cxi+_i+&consumer=ZHI_MENG) 为“松弛变量”， ![\xi _i=\max \left( 0,1-y_i\left( \boldsymbol{w}\cdot \boldsymbol{x}_{\boldsymbol{i}}+b \right) \right) ](https://www.zhihu.com/equation?tex=%5Cxi+_i%3D%5Cmax+%5Cleft%28+0%2C1-y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+%5Cright%29+&consumer=ZHI_MENG) ，即一个hinge损失函数。每一个样本都有一个对应的松弛变量，表征该样本不满足约束的程度。 ![ C>0 ](https://www.zhihu.com/equation?tex=+C%3E0+&consumer=ZHI_MENG) 称为惩罚参数， ![C](https://www.zhihu.com/equation?tex=C&consumer=ZHI_MENG) 值越大，对分类的惩罚越大。跟线性可分求解的思路一致，同样这里先用拉格朗日乘子法得到拉格朗日函数，再求其对偶问题。

综合以上讨论，我们可以得到**线性支持向量机学习算法**如下：

**输入**：训练数据集 ![ T=\left\{ \left( \boldsymbol{x}_1,y_1 \right) ,\left( \boldsymbol{x}_1,y_1 \right) ,...,\left( \boldsymbol{x}_N,y_N \right) \right\}](https://www.zhihu.com/equation?tex=+T%3D%5Cleft%5C%7B+%5Cleft%28+%5Cboldsymbol%7Bx%7D_1%2Cy_1+%5Cright%29+%2C%5Cleft%28+%5Cboldsymbol%7Bx%7D_1%2Cy_1+%5Cright%29+%2C...%2C%5Cleft%28+%5Cboldsymbol%7Bx%7D_N%2Cy_N+%5Cright%29+%5Cright%5C%7D&consumer=ZHI_MENG) 其中，![ \boldsymbol{x}_i\in \mathbb{R}^n ](https://www.zhihu.com/equation?tex=+%5Cboldsymbol%7Bx%7D_i%5Cin+%5Cmathbb%7BR%7D%5En+&consumer=ZHI_MENG)， ![ y_i\in \left\{ +1,-1 \right\} ,i=1,2,...N](https://www.zhihu.com/equation?tex=+y_i%5Cin+%5Cleft%5C%7B+%2B1%2C-1+%5Cright%5C%7D+%2Ci%3D1%2C2%2C...N&consumer=ZHI_MENG) ；

**输出**：分离超平面和分类决策函数

（1）选择惩罚参数 ![ C>0 ](https://www.zhihu.com/equation?tex=+C%3E0+&consumer=ZHI_MENG) ，构造并求解凸二次规划问题

![ \underset{\boldsymbol{\alpha }}{\min}\ \frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^N{\alpha _i\alpha _jy_iy_j\left( \boldsymbol{x}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{j}} \right)}}-\sum_{i=1}^N{\alpha _i}](https://www.zhihu.com/equation?tex=+%5Cunderset%7B%5Cboldsymbol%7B%5Calpha+%7D%7D%7B%5Cmin%7D%5C+%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%3D1%7D%5EN%7B%5Csum_%7Bj%3D1%7D%5EN%7B%5Calpha+_i%5Calpha+_jy_iy_j%5Cleft%28+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bj%7D%7D+%5Cright%29%7D%7D-%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_i%7D&consumer=ZHI_MENG)

![ s.t.\ \ \ \ \sum_{i=1}^N{\alpha _iy_i}=0](https://www.zhihu.com/equation?tex=+s.t.%5C+%5C+%5C+%5C+%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_iy_i%7D%3D0&consumer=ZHI_MENG)

![ \ \ \ \ \ \ \ 0\le \alpha _i\le C,\ i=1,2,...,N](https://www.zhihu.com/equation?tex=+%5C+%5C+%5C+%5C+%5C+%5C+%5C+0%5Cle+%5Calpha+_i%5Cle+C%2C%5C+i%3D1%2C2%2C...%2CN&consumer=ZHI_MENG)

得到最优解 ![ \boldsymbol{\alpha }^*=\left( \alpha _{1}^{*},\alpha _{2}^{*},...,\alpha _{N}^{*} \right) ^T ](https://www.zhihu.com/equation?tex=+%5Cboldsymbol%7B%5Calpha+%7D%5E%2A%3D%5Cleft%28+%5Calpha+_%7B1%7D%5E%7B%2A%7D%2C%5Calpha+_%7B2%7D%5E%7B%2A%7D%2C...%2C%5Calpha+_%7BN%7D%5E%7B%2A%7D+%5Cright%29+%5ET+&consumer=ZHI_MENG)

（2）计算

![ \boldsymbol{w}^*=\sum_{i=1}^N{\alpha _{i}^{*}y_i\boldsymbol{x}_i} ](https://www.zhihu.com/equation?tex=+%5Cboldsymbol%7Bw%7D%5E%2A%3D%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_%7Bi%7D%5E%7B%2A%7Dy_i%5Cboldsymbol%7Bx%7D_i%7D+&consumer=ZHI_MENG)

选择 ![\boldsymbol{\alpha }^* ](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7B%5Calpha+%7D%5E%2A+&consumer=ZHI_MENG) 的一个分量 $a_j^*$ 满足条件 ![ 0<\alpha _{j}^{*}<C ](https://www.zhihu.com/equation?tex=+0%3C%5Calpha+_%7Bj%7D%5E%7B%2A%7D%3CC+&consumer=ZHI_MENG) ，计算

![b^*=y_j-\sum_{i=1}^N{\alpha _{i}^{*}y_i\left( \boldsymbol{x}_{\boldsymbol{i}}\cdot \boldsymbol{x}_{\boldsymbol{j}} \right)}](https://www.zhihu.com/equation?tex=b%5E%2A%3Dy_j-%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_%7Bi%7D%5E%7B%2A%7Dy_i%5Cleft%28+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bj%7D%7D+%5Cright%29%7D&consumer=ZHI_MENG)

（3）求分离超平面

![ \boldsymbol{w}^*\cdot \boldsymbol{x}+b^*=0 ](https://www.zhihu.com/equation?tex=+%5Cboldsymbol%7Bw%7D%5E%2A%5Ccdot+%5Cboldsymbol%7Bx%7D%2Bb%5E%2A%3D0+&consumer=ZHI_MENG)

分类决策函数：

![ f\left( \boldsymbol{x} \right) =sign\left( \boldsymbol{w}^*\cdot \boldsymbol{x}+b^* \right) ](https://www.zhihu.com/equation?tex=+f%5Cleft%28+%5Cboldsymbol%7Bx%7D+%5Cright%29+%3Dsign%5Cleft%28+%5Cboldsymbol%7Bw%7D%5E%2A%5Ccdot+%5Cboldsymbol%7Bx%7D%2Bb%5E%2A+%5Cright%29+&consumer=ZHI_MENG)

#### 拉格朗日乘子法

[优化-拉格朗日乘子法 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/154517678)

用来解决优化问题

- 拉格朗日乘子法（Lagrange multipliers）是一种寻找多元函数**在一组约束下**的**极值**的方法。
- 通过引入拉格朗日乘子，可将有 d 个变量与 k个约束条件的最优化问题转化为具有 d+k个变量的无约束优化问题求解。

考虑下面这个优化问题

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230424201739033.png" alt="image-20230424201739033" style="zoom:50%;" />

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230424201650042.png" alt="image-20230424201650042" style="zoom:50%;" />

举个例子：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230424201839404.png" alt="image-20230424201839404" style="zoom:67%;" />

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230424201852362.png" alt="image-20230424201852362" style="zoom:67%;" />

### 正则化和不可分问题

在某些情况下，存在异常值，我们不清楚找到的超平面是否是我们想要的

图a表明一个最优边界分类器，

当我们在左上角添加一个样本，它就会导致决策边界发生戏剧性的摆动，

在图c的情况下，数据不能完全线性可分。

![image-20230426172752560](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230426172752560.png)

为了使算法适用于非线性可分离的数据集，并且对异常值不那么敏感，我们在约束（硬边界->软边界）中引入了正松弛变量

![image-20230426173319684](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230426173319684.png)

一种为了错误分配的额外的代价的自然方法如下	

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230426173418638.png" alt="image-20230426173418638" style="zoom:50%;" />

c是用户选择的参数，较大的c表示对错误的惩罚的大

将参数C定义为对间隔进行松弛$ξ$时的惩罚项。当C越大时，越不允许有过大的$ξ$，因此分割平面必须越能正确的区分每个数据点，所以分割平面越复杂，越可能导致过拟合。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230426175030587.png" alt="image-20230426175030587" style="zoom:67%;" />

为了简单起见，我们设置c=1

我们将优化（$l_1$正则化）的表示如下：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230426173708705.png" alt="image-20230426173708705" style="zoom:67%;" />

[(71条消息) 72-Soft Margin和SVM的正则化_svm正则化_蓝子娃娃的博客-CSDN博客](https://blog.csdn.net/qq_41033011/article/details/109214089)

## 非线性分类问题

最理想的状态是样本在向量空间中是线性可分的，我们可以清晰的分隔成不同的类别——线性可分ＳＶＭ

如果面对的分类问题，不是线性的

<img src="https://pic3.zhimg.com/80/v2-1c4f3736f8a5577b6cf7f8f0afb520ba_720w.webp" alt="img" style="zoom:67%;" />

圆圈在二维空间中无法用线性函数表示，在二维空间中不可以线性分割

**我们可以在更高维的空间内线性可分**

如图文件将正负类的样本映射到三维空间中，并且一句不同的类被给他们赋予不同的高度值（ｚ值），这样就线性可分了

![img](https://pic1.zhimg.com/80/v2-afec0bbe6be96331bd6a6ecd59cb5ed8_720w.webp)

中间用一个超平面来分割

对于有限维度向量空间中不可线性分割的样本，我们就将其映射到更高维度的向量空间内，在通过间隔最大化的方式，学习得到支持向量机，**就是非线性SVM**

我们将样本映射到这个更高维度的新空间就叫做特征空间

**非线性SVM＝核技巧＋线性SVM**

这里面的$K(x_i,x_j)=\phi(x_i)^T\phi(x_j)$就是核函数(kerne fucntion)

![image-20230506163735591](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230506163735591.png)

### 核技巧kernel trick

基本思想就是将低维空间不可数据映射到高维空间

$\phi(x)$的计算成本是很高的

![image-20230506164604650](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230506164604650.png)

![image-20230506164732095](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230506164732095.png)

![image-20230506164825382](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230506164825382.png)

![image-20230506165215670](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230506165215670.png)

![image-20230506171518731](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230506171518731.png)

**common kernel**

Linear kernel 

安全，快速，可解释

受限的（总是不可分离的）

Polynomial kernel 多项式核

比线性限制少

数值困难对于大的d，

三个参数难以选择

Radial basis function kernel (高斯核)

比线性/多项式更加强大

一个参数，更加容易来选择

神秘，缓慢，可能很强大

最受欢迎的之一，但是小心翼翼

![image-20230506173524635](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230506173524635.png)

•核的思想比支持向量机具有更广泛的适用性。

•如果你有任何学习算法，你可以只用输入向量$x_ix_j$之间的内积来写，那么用$K(x_i,x_j)$来代替它，其中K是一个核。

•你可以“神奇地”让你的算法在对应于K的高维特征空间中高效地工作。

•标准线性算法可以通过特征空间推广到其非线性版本

•核PCA

•核独立分量分析(ICA)

•核典型相关分析(CCA)

•核k-means
